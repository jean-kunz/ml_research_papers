{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fe9f068",
   "metadata": {},
   "source": [
    "# Every Model Learned by Gradient Descent Is Approximately a Kernel Machine\n",
    "https://arxiv.org/pdf/2012.00152"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5097cde",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3803c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd5022b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=128, output_size=10):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a22d164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathKernelTracker:\n",
    "    \"\"\"\n",
    "    Tracks gradients during training to compute path kernels\n",
    "    This demonstrates the paper's main theorem\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.gradient_history = []\n",
    "        self.training_examples = []\n",
    "        self.training_labels = []\n",
    "\n",
    "    def store_gradients_after_backward(self):\n",
    "        \"\"\"Store gradients after backward pass (called from main training loop)\"\"\"\n",
    "        # Collect gradients as a flat vector\n",
    "        gradients = []\n",
    "        for param in self.model.parameters():\n",
    "            if param.grad is not None:\n",
    "                gradients.append(param.grad.view(-1).clone())\n",
    "\n",
    "        if gradients:\n",
    "            gradient_vector = torch.cat(gradients)\n",
    "        else:\n",
    "            gradient_vector = torch.zeros(\n",
    "                sum(p.numel() for p in self.model.parameters())\n",
    "            )\n",
    "\n",
    "        self.gradient_history.append(gradient_vector)\n",
    "\n",
    "    def store_training_example(self, x, y):\n",
    "        \"\"\"Store training examples\"\"\"\n",
    "        self.training_examples.append(x.clone())\n",
    "        self.training_labels.append(y.clone())\n",
    "\n",
    "    def compute_path_kernel(self, x_query, x_train_idx):\n",
    "        \"\"\"\n",
    "        Compute path kernel between query point and a training example\n",
    "        K(x, x') = ∫ ∇y(x) · ∇y(x') dt over the path\n",
    "\n",
    "        We approximate this as: Σ_t ∇y(x)|_t · ∇y(x')|_t\n",
    "        \"\"\"\n",
    "        if x_train_idx >= len(self.gradient_history):\n",
    "            return 0.0\n",
    "\n",
    "        # For simplicity, we'll use the stored gradient from training\n",
    "        # In practice, we'd need to compute gradients for the query at each training step\n",
    "        train_grad = self.gradient_history[x_train_idx]\n",
    "\n",
    "        # Compute gradients for query point (using current model state)\n",
    "        self.model.eval()\n",
    "        with torch.enable_grad():\n",
    "            # Create a copy of query that requires grad\n",
    "            x_query_copy = x_query.clone().detach()\n",
    "            # We'll approximate by using parameter gradients at current state\n",
    "            temp_model = type(self.model)()\n",
    "            temp_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "            temp_model.zero_grad()\n",
    "            output = temp_model(x_query_copy.unsqueeze(0))\n",
    "            # Use mean of output for gradient computation\n",
    "            dummy_loss = output.mean()\n",
    "            dummy_loss.backward()\n",
    "\n",
    "            query_grad = []\n",
    "            for param in temp_model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    query_grad.append(param.grad.view(-1))\n",
    "\n",
    "            if query_grad:\n",
    "                query_grad = torch.cat(query_grad)\n",
    "                kernel_value = torch.dot(query_grad, train_grad).item()\n",
    "                return kernel_value\n",
    "            else:\n",
    "                return 0.0\n",
    "\n",
    "    def predict_as_kernel_machine(self, x_query):\n",
    "        \"\"\"\n",
    "        Make prediction using kernel machine formulation:\n",
    "        y = Σ_i a_i * K(x, x_i) + b\n",
    "        \"\"\"\n",
    "        if len(self.training_examples) == 0:\n",
    "            return torch.zeros(10)  # Return zero prediction\n",
    "\n",
    "        prediction = torch.zeros(10)\n",
    "\n",
    "        for i, (x_train, y_train) in enumerate(\n",
    "            zip(self.training_examples, self.training_labels)\n",
    "        ):\n",
    "            # Compute path kernel\n",
    "            kernel_value = self.compute_path_kernel(x_query, i)\n",
    "\n",
    "            # Create one-hot encoding for y_train\n",
    "            y_onehot = torch.zeros(10)\n",
    "            y_onehot[y_train] = 1.0\n",
    "\n",
    "            # Weight by kernel value (simplified, in practice a_i depends on loss derivatives)\n",
    "            weight = kernel_value / len(self.training_examples)  # Normalize\n",
    "            prediction += weight * y_onehot\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edc230bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_path_kernel():\n",
    "    # Load a small subset of MNIST\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "\n",
    "    full_dataset = datasets.MNIST(\n",
    "        \"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    # Use only first 100 examples for demonstration\n",
    "    small_dataset = Subset(full_dataset, range(100))\n",
    "    train_loader = DataLoader(small_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Test set\n",
    "    test_dataset = datasets.MNIST(\"./data\", train=False, transform=transform)\n",
    "    test_subset = Subset(test_dataset, range(10))  # Just 10 test examples\n",
    "    test_loader = DataLoader(test_subset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Initialize model and tracker\n",
    "    model = SimpleNet()\n",
    "    tracker = PathKernelTracker(model)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    print(\"Training neural network and tracking path kernels...\")\n",
    "\n",
    "    # Training loop with path kernel tracking\n",
    "    model.train()\n",
    "    for epoch in range(3):  # Just a few epochs for demo\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Store training example\n",
    "            tracker.store_training_example(data, target)\n",
    "\n",
    "            # Regular training step\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            # Store gradients after backward pass\n",
    "            tracker.store_gradients_after_backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(\"\\nComparing neural network predictions vs kernel machine predictions...\")\n",
    "\n",
    "    # Test both approaches\n",
    "    model.eval()\n",
    "    correct_nn = 0\n",
    "    correct_km = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            # Neural network prediction\n",
    "            nn_output = model(data)\n",
    "            nn_pred = nn_output.argmax(dim=1)\n",
    "\n",
    "            # Kernel machine prediction using path kernels\n",
    "            km_output = tracker.predict_as_kernel_machine(data.squeeze(0))\n",
    "            km_pred = km_output.argmax()\n",
    "\n",
    "            correct_nn += (nn_pred == target).sum().item()\n",
    "            correct_km += (km_pred == target).sum().item()\n",
    "\n",
    "            print(\n",
    "                f\"Example {i}: True={target.item()}, NN_pred={nn_pred.item()}, KM_pred={km_pred.item()}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  NN confidence: {F.softmax(nn_output, dim=1)[0, nn_pred].item():.3f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  KM confidence: {F.softmax(km_output.unsqueeze(0), dim=1)[0, km_pred].item():.3f}\"\n",
    "            )\n",
    "\n",
    "            if i >= 5:  # Limit output for readability\n",
    "                break\n",
    "\n",
    "    print(f\"\\nAccuracy comparison (limited test set):\")\n",
    "    print(\n",
    "        f\"Neural Network: {correct_nn}/{min(6, len(test_loader))} = {correct_nn / min(6, len(test_loader)):.3f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Kernel Machine: {correct_km}/{min(6, len(test_loader))} = {correct_km / min(6, len(test_loader)):.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41689f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural network and tracking path kernels...\n",
      "Epoch: 0, Batch: 0, Loss: 2.3234\n",
      "Epoch: 0, Batch: 20, Loss: 2.0697\n",
      "Epoch: 0, Batch: 40, Loss: 1.3557\n",
      "Epoch: 0, Batch: 60, Loss: 1.5063\n",
      "Epoch: 0, Batch: 80, Loss: 3.3740\n",
      "Epoch: 1, Batch: 0, Loss: 3.7886\n",
      "Epoch: 1, Batch: 20, Loss: 0.1847\n",
      "Epoch: 1, Batch: 40, Loss: 0.2206\n",
      "Epoch: 1, Batch: 60, Loss: 0.3959\n",
      "Epoch: 1, Batch: 80, Loss: 2.3313\n",
      "Epoch: 2, Batch: 0, Loss: 2.6912\n",
      "Epoch: 2, Batch: 20, Loss: 0.0251\n",
      "Epoch: 2, Batch: 40, Loss: 0.0434\n",
      "Epoch: 2, Batch: 60, Loss: 0.1141\n",
      "Epoch: 2, Batch: 80, Loss: 0.6814\n",
      "\n",
      "Comparing neural network predictions vs kernel machine predictions...\n",
      "Example 0: True=7, NN_pred=7, KM_pred=7\n",
      "  NN confidence: 0.967\n",
      "  KM confidence: 0.114\n",
      "Example 1: True=2, NN_pred=2, KM_pred=2\n",
      "  NN confidence: 0.425\n",
      "  KM confidence: 0.106\n",
      "Example 2: True=1, NN_pred=1, KM_pred=1\n",
      "  NN confidence: 0.964\n",
      "  KM confidence: 0.111\n",
      "Example 3: True=0, NN_pred=0, KM_pred=7\n",
      "  NN confidence: 0.842\n",
      "  KM confidence: 0.114\n",
      "Example 4: True=4, NN_pred=4, KM_pred=7\n",
      "  NN confidence: 0.823\n",
      "  KM confidence: 0.105\n",
      "Example 5: True=1, NN_pred=1, KM_pred=1\n",
      "  NN confidence: 0.978\n",
      "  KM confidence: 0.115\n",
      "\n",
      "Accuracy comparison (limited test set):\n",
      "Neural Network: 6/6 = 1.000\n",
      "Kernel Machine: 4/6 = 0.667\n"
     ]
    }
   ],
   "source": [
    "demonstrate_path_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3efc0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradient_similarity():\n",
    "    \"\"\"\n",
    "    Visualize how similar gradients lead to similar predictions\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"VISUALIZING GRADIENT SIMILARITY\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Simple 2D example for visualization\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Create simple 2D data\n",
    "    X = torch.randn(20, 2)\n",
    "    y = (X[:, 0] + X[:, 1] > 0).long()  # Simple linear boundary\n",
    "\n",
    "    # Simple model\n",
    "    model = nn.Linear(2, 2)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    # Store gradients during training\n",
    "    gradients_history = []\n",
    "\n",
    "    print(\"Training on 2D data and collecting gradients...\")\n",
    "\n",
    "    for epoch in range(10):\n",
    "        for i in range(len(X)):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X[i : i + 1])\n",
    "            loss = F.cross_entropy(output, y[i : i + 1])\n",
    "            loss.backward()\n",
    "\n",
    "            # Store gradients\n",
    "            grads = []\n",
    "            for param in model.parameters():\n",
    "                grads.append(param.grad.clone().flatten())\n",
    "            gradients_history.append(torch.cat(grads))\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "    # Compute gradient similarities (simplified path kernel)\n",
    "    print(\"\\nGradient similarity matrix (shows path kernel values):\")\n",
    "    n_samples = min(5, len(X))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            # Average gradient similarity over training\n",
    "            similarities = []\n",
    "            for step in range(0, len(gradients_history), len(X)):\n",
    "                if step + i < len(gradients_history) and step + j < len(\n",
    "                    gradients_history\n",
    "                ):\n",
    "                    grad_i = gradients_history[step + i]\n",
    "                    grad_j = gradients_history[step + j]\n",
    "                    sim = torch.dot(grad_i, grad_j).item()\n",
    "                    similarities.append(sim)\n",
    "\n",
    "            avg_sim = np.mean(similarities) if similarities else 0\n",
    "            print(f\"K(x_{i}, x_{j}) = {avg_sim:.3f}\", end=\"  \")\n",
    "        print()\n",
    "\n",
    "    print(f\"\\nData points and labels:\")\n",
    "    for i in range(n_samples):\n",
    "        print(\n",
    "            f\"x_{i}: [{X[i, 0].item():.2f}, {X[i, 1].item():.2f}] -> label: {y[i].item()}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a51c08c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "VISUALIZING GRADIENT SIMILARITY\n",
      "==================================================\n",
      "Training on 2D data and collecting gradients...\n",
      "\n",
      "Gradient similarity matrix (shows path kernel values):\n",
      "K(x_0, x_0) = 0.590  K(x_0, x_1) = 0.037  K(x_0, x_2) = -0.030  K(x_0, x_3) = 0.062  K(x_0, x_4) = 0.182  \n",
      "K(x_1, x_0) = 0.037  K(x_1, x_1) = 0.869  K(x_1, x_2) = 0.565  K(x_1, x_3) = 0.257  K(x_1, x_4) = 0.483  \n",
      "K(x_2, x_0) = -0.030  K(x_2, x_1) = 0.565  K(x_2, x_2) = 0.512  K(x_2, x_3) = 0.157  K(x_2, x_4) = 0.258  \n",
      "K(x_3, x_0) = 0.062  K(x_3, x_1) = 0.257  K(x_3, x_2) = 0.157  K(x_3, x_3) = 0.092  K(x_3, x_4) = 0.103  \n",
      "K(x_4, x_0) = 0.182  K(x_4, x_1) = 0.483  K(x_4, x_2) = 0.258  K(x_4, x_3) = 0.103  K(x_4, x_4) = 0.762  \n",
      "\n",
      "Data points and labels:\n",
      "x_0: [1.93, 1.49] -> label: 1\n",
      "x_1: [0.90, -2.11] -> label: 0\n",
      "x_2: [0.68, -1.23] -> label: 0\n",
      "x_3: [-0.04, -1.60] -> label: 0\n",
      "x_4: [-0.75, 1.65] -> label: 1\n"
     ]
    }
   ],
   "source": [
    "visualize_gradient_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2de354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-research-papers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
