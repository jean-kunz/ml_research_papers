{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effective Use of Word Order for Text Categorization with Convolutional Neural Networks\n",
    " \n",
    "https://arxiv.org/abs/1412.1058\n",
    "\n",
    "\n",
    "WORK IN PROGRESS !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile,os,sys, re\n",
    "is_eager_exec_init=False\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#if not is_eager_exec_init:\n",
    "#    tf.enable_eager_execution()\n",
    "#    is_eager_exec_init=True\n",
    "\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import pickle\n",
    "import sklearn\n",
    "\n",
    "# Use spacy to remove stop words first\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'memory_profiler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e7ed4321e242>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'memory_profiler'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'memit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2129\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2130\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2131\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-65>\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/magics/extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Missing module name.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'already loaded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'memory_profiler'"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "%memit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_data_already_prepared = False\n",
    "do_prepare_data=False\n",
    "do_load_prepared_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datasets/aclImdb_v1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!ls /datasets/aclImdb_v1.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_data_already_prepared:\n",
    "    test_neg_files = []\n",
    "    test_pos_files = []\n",
    "    train_neg_files = []\n",
    "    train_pos_files = []\n",
    "\n",
    "    with tarfile.open('/datasets/aclImdb_v1.tar.gz') as tar:\n",
    "        for mbr in tar.getmembers():\n",
    "            matches = re.findall('/test/neg/.*\\.txt',mbr.name)\n",
    "            if len(matches)==1:\n",
    "                f=tar.extractfile(mbr)\n",
    "                content=f.read()        \n",
    "                test_neg_files.append((mbr,content))\n",
    "            matches = re.findall('/test/pos/.*\\.txt',mbr.name)\n",
    "            if len(matches)==1:\n",
    "                f=tar.extractfile(mbr)\n",
    "                content=f.read()  \n",
    "                test_pos_files.append((mbr, content))\n",
    "            matches = re.findall('/train/neg/.*\\.txt',mbr.name)\n",
    "            if len(matches)==1:\n",
    "                f=tar.extractfile(mbr)\n",
    "                content=f.read()        \n",
    "                train_neg_files.append((mbr,content))\n",
    "            matches = re.findall('/train/pos/.*\\.txt',mbr.name)\n",
    "            if len(matches)==1:\n",
    "                f=tar.extractfile(mbr)\n",
    "                content=f.read()  \n",
    "                train_pos_files.append((mbr, content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_data_already_prepared:\n",
    "    train_neg = [txt.decode(\"utf-8\")  for file,txt in train_neg_files]\n",
    "    train_pos = [txt.decode(\"utf-8\")  for file,txt in train_pos_files]\n",
    "\n",
    "    test_neg = [txt.decode(\"utf-8\")  for file,txt in test_neg_files]\n",
    "    test_pos = [txt.decode(\"utf-8\")  for file,txt in test_pos_files]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%memit\n",
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_nb = 10000\n",
    "\n",
    "def clean_docs(docs):\n",
    "    # clean docs by:\n",
    "    # - removing stop words\n",
    "    docs_wtho_stop = []\n",
    "    for raw_doc in docs:\n",
    "        doc = nlp(raw_doc)\n",
    "        doc_wtho_stop = \"\"\n",
    "        for tok in doc:\n",
    "            if not tok.is_stop:\n",
    "                doc_wtho_stop = doc_wtho_stop +tok.text+ \" \"\n",
    "        docs_wtho_stop.append(doc_wtho_stop)\n",
    "    return docs_wtho_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer, one_hot\n",
    "from tensorflow.python.keras.backend import one_hot, get_session, set_session\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Conv1D,MaxPool1D, Activation, Flatten\n",
    "from tensorflow.python.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from tensorflow.python.keras.regularizers import l2, l1\n",
    "from tensorflow.python.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_to_sequences(docs, vocab_nb=10000, max_length=None):\n",
    "    # create the tokenizer\n",
    "    t = Tokenizer(num_words=vocab_nb)\n",
    "    # fit the tokenizer on the documents\n",
    "    t.fit_on_texts(docs)\n",
    "    # create each doc as a list of integer indices\n",
    "    docs_seq_int = t.texts_to_sequences(docs)\n",
    "    #print(docs_seq_int)\n",
    "    \n",
    "    \n",
    "    doc_nb = len(docs_seq_int)\n",
    "    if max_length==None:\n",
    "        max_length = len(sorted(docs_seq_int,key=len, reverse=True)[0])\n",
    "        print(\"max_lenght computed:\",max_length)\n",
    "    #print(\"max_length:\",max_length)\n",
    "    \n",
    "    # Create a dense array of shape (doc_nb,max_length). \n",
    "    # Each cell contains a int (the indice of word)\n",
    "    \n",
    "    docs_seq_dense = []\n",
    "    docs_seq_array = np.zeros((doc_nb,max_length),dtype=np.int16)\n",
    "    #print(docs_seq_array)\n",
    "    for idx, seq_int in enumerate(docs_seq_int):\n",
    "        padded_seq_int = seq_int+[0]*(max_length-len(seq_int))\n",
    "        doc_array = np.array(padded_seq_int, dtype=np.int16)\n",
    "        docs_seq_array[idx]=doc_array[:max_length]\n",
    "\n",
    "    return docs_seq_array\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Test data preparation\n",
    "\n",
    "Data preparation logic is tested on a very small toy data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Well ! ', 'Good work , good effort ', 'Great effort guys ', 'nice work friend ', 'Excellent job friend ! ', 'What poor job ! ! ', 'This shame ', 'You poor guy ', 'Fantastic effort friend brother , hell great job I like ', 'I like friend ', 'You worked bad poor guy ']\n",
      "[[12  0  0  0  0  0  0  0]\n",
      " [ 5  6  5  2  0  0  0  0]\n",
      " [ 7  2 13  0  0  0  0  0]\n",
      " [14  6  1  0  0  0  0  0]\n",
      " [ 3  1  0  0  0  0  0  0]\n",
      " [ 4  3  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 8  4  9  0  0  0  0  0]\n",
      " [ 2  1  7  3 10 11  0  0]\n",
      " [10 11  1  0  0  0  0  0]\n",
      " [ 8  4  9  0  0  0  0  0]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[12  0  0  0  0  0  0  0]\n",
      " [ 5  6  5  2  0  0  0  0]\n",
      " [ 7  2 13  0  0  0  0  0]\n",
      " [14  6  1  0  0  0  0  0]\n",
      " [ 3  1  0  0  0  0  0  0]\n",
      " [ 4  3  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 8  4  9  0  0  0  0  0]\n",
      " [ 2  1  7  3 10 11  0  0]\n",
      " [10 11  1  0  0  0  0  0]\n",
      " [ 8  4  9  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "do_toy_test=True\n",
    "\n",
    "if do_toy_test:\n",
    "    toy_vocab_nb = 15\n",
    "    toy_max_sent_len = 8\n",
    "    docs = ['Well done!',\n",
    "            'Good work, good effort',\n",
    "            'Great effort guys',\n",
    "            'nice work my friend',\n",
    "            'Excellent job my friend!',\n",
    "            'What a poor job !! ',\n",
    "            'This is a shame',\n",
    "            'You are a poor guy',\n",
    "            'Fantastic effort my friend and brother, hell of a great job for ever I like it',\n",
    "            'I like you so much my friend',\n",
    "            'You worked so bad poor guy']\n",
    "    cleaned_docs = clean_docs(docs)\n",
    "    print(cleaned_docs)\n",
    "    \n",
    "    doc_sequences = docs_to_sequences(cleaned_docs, max_length=toy_max_sent_len, vocab_nb=toy_vocab_nb)\n",
    "    print(doc_sequences)\n",
    "    print(type(doc_sequences))\n",
    "    np.save(\"./tst\",doc_sequences)\n",
    "    tmp_array = np.load(\"./tst.npy\")\n",
    "    print(tmp_array)\n",
    "    \n",
    "    labels=[1,1,0,1,1,0,0,0,1,0,1]\n",
    "#%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_generator(X, y, batch_size=64,vocab_nb=10000):\n",
    "    # Must be able to iterate many times on the same X/y , for instance while training with many epochs.\n",
    "    i=0\n",
    "    while True:\n",
    "        #print(i)\n",
    "        #print(i)\n",
    "        if i >= len(X):\n",
    "            i=0\n",
    "        tmp_X = X[i:batch_size+i]\n",
    "        #print(tmp_X)\n",
    "        batch_X= to_categorical(tmp_X,num_classes=vocab_nb)\n",
    "        batch_y = y[i:batch_size+i]\n",
    "        #print(\"cat gen: \", len(batch_X), len(batch_y))\n",
    "        yield batch_X, batch_y\n",
    "        i+=batch_size\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_toy_test:\n",
    "    #print(doc_sequences,labels)\n",
    "    toy_batch_size=2\n",
    "    nb_iter=doc_sequences.shape[0] // toy_batch_size\n",
    "    j=0\n",
    "    nb_epoch=0\n",
    "    for batch_x, batch_y in categorical_generator(doc_sequences,labels,batch_size=toy_batch_size,vocab_nb=toy_vocab_nb):\n",
    "        #print(type(batch_x),batch_x.shape)\n",
    "        #print(batch_y)\n",
    "        assert (len(batch_x)<=toy_batch_size) & (len(batch_y) <=toy_batch_size)\n",
    "        #print(nb_epoch,j,nb_iter)\n",
    "        if j > nb_iter:\n",
    "            j=0\n",
    "            nb_epoch+=1\n",
    " #       if batch_x.shape[0]<test_batch_size:\n",
    " #           nb_epoch+=1\n",
    "        if nb_epoch ==3:\n",
    "            break\n",
    "        \n",
    "        j+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/initializers.py:104: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n",
      "nb steps per epoch 3.6666666666666665\n",
      "Epoch 1/10\n",
      " - 3s - loss: 0.6603 - acc: 0.5833 - val_loss: 0.9370 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6728 - acc: 0.5833 - val_loss: 0.9388 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6426 - acc: 0.6667 - val_loss: 0.9444 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6531 - acc: 0.7500 - val_loss: 0.9472 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6674 - acc: 0.6667 - val_loss: 0.9486 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6377 - acc: 0.7500 - val_loss: 0.9530 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.6481 - acc: 0.8333 - val_loss: 0.9556 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6629 - acc: 0.7500 - val_loss: 0.9570 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6334 - acc: 0.7500 - val_loss: 0.9625 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6433 - acc: 0.8333 - val_loss: 0.9652 - val_acc: 0.0000e+00\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 7, 1)              31        \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 3, 1)              0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 35\n",
      "Trainable params: 35\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "do_toy_test=True\n",
    "if do_toy_test:\n",
    "    #sess = get_session()\n",
    "    #docs_as_one_hot=sess.run(docs_as_one_hot_tensor)\n",
    "    #docs_as_one_hot=to_categorical(doc_sequences)\n",
    "    # try with tf_data\n",
    "    #print(doc_as_one_hot)\n",
    "    toy_batch_size=3\n",
    "    toy_model = Sequential([\n",
    "        Conv1D(1, 2, strides=1, activation='relu', kernel_initializer='he_normal', bias_initializer='zeros',input_shape=(toy_max_sent_len,toy_vocab_nb), kernel_regularizer=l2(0.01)),\n",
    "        MaxPool1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(1, kernel_initializer='he_normal', bias_initializer='zeros', activation='sigmoid')\n",
    "    ])\n",
    "    toy_model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    tb_cb = TensorBoard(log_dir='./logs-test', histogram_freq=0, batch_size=4, \n",
    "                write_graph=True, write_grads=True, )\n",
    "    \n",
    "    test_steps_per_epoch=len(doc_sequences)/toy_batch_size\n",
    "    print(\"nb steps per epoch\", test_steps_per_epoch)\n",
    "    train_cat_gen = categorical_generator(doc_sequences[:-2],labels[:-2],batch_size=toy_batch_size,vocab_nb=toy_vocab_nb)\n",
    "    xval_cat_gen = categorical_generator(doc_sequences[-2:],labels[-2:],batch_size=toy_batch_size,vocab_nb=toy_vocab_nb)\n",
    "    toy_history = toy_model.fit_generator(train_cat_gen, steps_per_epoch=test_steps_per_epoch,epochs=10, validation_data=xval_cat_gen,validation_steps=1 ,verbose=2, callbacks=[tb_cb])\n",
    "    \n",
    "    toy_model.summary()\n",
    "    toy_model.save('./toy_model_v1')\n",
    "    \n",
    "#%memit\n",
    "#psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(is_data_already_prepared)\n",
    "is_data_already_prepared=True\n",
    "max_sent_len = 1000\n",
    "if not is_data_already_prepared:\n",
    "    \n",
    "    cleaned_train_neg = clean_docs(train_neg)\n",
    "    seq_train_neg = docs_to_sequences(cleaned_train_neg, max_length=max_sent_len)    \n",
    "    cleaned_train_pos = clean_docs(train_pos)\n",
    "    seq_train_pos = docs_to_sequences(cleaned_train_pos, max_length=max_sent_len)\n",
    "    np.save('./seq_train_neg',seq_train_neg)\n",
    "    np.save('./seq_train_pos',seq_train_pos)\n",
    "    \n",
    "    cleaned_test_neg = clean_docs(test_neg)\n",
    "    seq_test_neg = docs_to_sequences(cleaned_test_neg,max_length=max_sent_len)\n",
    "    cleaned_test_pos = clean_docs(test_pos)\n",
    "    seq_test_pos = docs_to_sequences(cleaned_test_pos, max_length=max_sent_len)\n",
    "    np.save('./seq_test_neg',seq_test_neg)\n",
    "    np.save('./seq_test_pos',seq_test_pos)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(cleaned_train_pos)\n",
    "# return array of list only if max_length is set.\n",
    "tmp_pos_no_len = docs_to_sequences(cleaned_test_pos,max_length=1000, vocab_nb=vocab_nb)\n",
    "#tmp_pos_no_len = docs_to_sequences(cleaned_test_pos)\n",
    "\n",
    "tmp_neg = docs_to_sequences(cleaned_train_neg[:10],max_length=max_len)\n",
    "tmp_pos = docs_to_sequences(cleaned_train_pos[:10],max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared seq loaded\n"
     ]
    }
   ],
   "source": [
    "is_data_already_prepared=True\n",
    "if is_data_already_prepared:\n",
    "    seq_test_neg = np.load('./seq_test_neg.npy')\n",
    "    seq_test_pos = np.load('./seq_test_pos.npy')\n",
    "    seq_train_neg = np.load('./seq_train_neg.npy')\n",
    "    seq_train_pos = np.load('./seq_train_pos.npy')\n",
    "    print(\"prepared seq loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12500, 1000), (12500, 1000), (12500, 1000), (12500, 1000))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.array(seq_train_neg)\n",
    "seq_train_neg[:].shape, seq_train_pos[:].shape,seq_test_neg.shape, seq_test_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_train_neg.shape, type(seq_test_neg)\n",
    "train_pos = np.ones((seq_train_neg.shape[0]))\n",
    "train_neg = np.zeros((seq_train_neg.shape[0]))\n",
    "\n",
    "test_pos = np.ones((seq_test_neg.shape[0]))\n",
    "test_neg = np.zeros((seq_test_neg.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12500, 1000), (25000, 1000), (25000, 1000), (25000,), (25000,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seq_train = seq_train_neg + seq_train_pos\n",
    "seq_train = np.append(seq_train_neg,seq_train_pos,axis=0)\n",
    "seq_test = np.append(seq_test_neg,seq_test_pos,axis=0)\n",
    "train_label = np.append(train_neg,train_pos,axis=0)\n",
    "test_label = np.append(test_neg,test_pos,axis=0)\n",
    "assert np.any(train_label[:12500]==0)\n",
    "assert np.any(train_label[12500:]==1.)\n",
    "assert np.any(test_label[:12500]==0)\n",
    "assert np.any(test_label[12500:]==1.)\n",
    "seq_train_neg.shape, seq_train.shape,seq_test.shape,train_label.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 1000), (5000, 1000), (20000,), (5000,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xval_rate=0.2\n",
    "X_train, X_xval, y_train, y_xval = sklearn.model_selection.train_test_split(seq_train,train_label, test_size=0.2)\n",
    "assert X_train.shape==(seq_train.shape[0] * (1-xval_rate),seq_train.shape[1])\n",
    "X_train.shape,X_xval.shape, y_train.shape, y_xval.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_6 (Conv1D)            (None, 999, 1)            20001     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 499, 1)            0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 499)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 500       \n",
      "=================================================================\n",
      "Total params: 20,501\n",
      "Trainable params: 20,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "nb steps per epoch 312\n"
     ]
    }
   ],
   "source": [
    "do_train=True\n",
    "if do_train:\n",
    "\n",
    "    batch_size=64\n",
    "    model = Sequential([\n",
    "        Conv1D(1, 2, strides=1, activation='relu', kernel_initializer='he_normal', bias_initializer='zeros',input_shape=(max_sent_len,vocab_nb), kernel_regularizer=l2(0.01)),\n",
    "        MaxPool1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(1, kernel_initializer='he_normal', bias_initializer='zeros',activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    tb_cb = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=4, \n",
    "                write_graph=True, write_grads=True, )\n",
    "    # used to train only on a subset while configuring the network architecture\n",
    "    train_nb = 128\n",
    "    train_nb = None\n",
    "    if not train_nb:\n",
    "        train_nb = len(X_train)\n",
    "    xval_nb = 20\n",
    "    xval_nb = None\n",
    "    if not xval_nb:\n",
    "        xval_nb = len(X_xval)\n",
    "    epochs_nb = 10  #100\n",
    "    steps_per_epoch=train_nb// batch_size\n",
    "    print(\"nb steps per epoch\", steps_per_epoch)\n",
    "    train_cat_gen = categorical_generator(X_train[:train_nb],y_train[:train_nb],batch_size=batch_size,vocab_nb=vocab_nb)\n",
    "    xval_cat_gen = categorical_generator(X_xval[:xval_nb],y_xval[:xval_nb],batch_size=batch_size,vocab_nb=vocab_nb)\n",
    "    history = model.fit_generator(train_cat_gen, steps_per_epoch=steps_per_epoch,epochs=epochs_nb, validation_data=xval_cat_gen,validation_steps=1 ,verbose=0, callbacks=[tb_cb])\n",
    "    \n",
    "    \n",
    "    model.save('./model_v1')\n",
    "    model.save_weights('./model_w_v1')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nb = None\n",
    "if not train_nb:\n",
    "    train_nb = len(X_train)\n",
    "print(train_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try with  deep pyramid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "do_train=True\n",
    "if do_train:\n",
    "\n",
    "    batch_size=64\n",
    "    model = Sequential([\n",
    "        Conv1D(1, 2, strides=1, activation='relu', input_shape=(max_sent_len,vocab_nb), kernel_regularizer=l2(0.01)),\n",
    "        MaxPool1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    tb_cb = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=4, \n",
    "                write_graph=True, write_grads=True, )\n",
    "    \n",
    "    steps_per_epoch=len(X_train)// batch_size\n",
    "    print(\"nb steps per epoch\", steps_per_epoch)\n",
    "    train_cat_gen = categorical_generator(X_train[:],y_train[:],batch_size=batch_size,vocab_nb=vocab_nb)\n",
    "    xval_cat_gen = categorical_generator(X_xval[:],y_xval[:],batch_size=batch_size,vocab_nb=vocab_nb)\n",
    "    history = model.fit_generator(train_cat_gen, steps_per_epoch=steps_per_epoch,epochs=50, validation_data=xval_cat_gen,validation_steps=1 ,verbose=2, callbacks=[tb_cb])\n",
    "    model.summary()\n",
    "    \n",
    "    model.save_weights()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7fffffffffffffff True\n",
      "7fffffffffffffff False\n",
      "Is 64 bits: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.5.2 (default, Nov 23 2017, 16:37:01) \\n[GCC 5.4.0 20160609]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys;print(\"%x\" % sys.maxsize, sys.maxsize > 2**32)\n",
    "import sys;print(\"%x\" % sys.maxsize, sys.maxsize > 2**64)\n",
    "sys.maxsize\n",
    "import platform\n",
    "platform.architecture()\n",
    "import sys\n",
    "is_64bits = sys.maxsize > 2**32\n",
    "print(\"Is 64 bits:\",is_64bits)\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%memit\n",
    "print(vocab_nb)\n",
    "\n",
    "train_neg_docs_as_one_hot = to_categorical(seq_train_neg[:500], num_classes=vocab_nb)\n",
    "%memit\n",
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_docs_as_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_neg_docs_as_one_hot_tensor = one_hot(seq_train_neg,num_classes=vocab_nb)\n",
    "#train_pos_docs_as_one_hot_tensor = one_hot(seq_train_pos,num_classes=vocab_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess = get_session()\n",
    "#train_neg_docs_as_one_hot=sess.run(train_neg_docs_as_one_hot_tensor)\n",
    "#train_pos_docs_as_one_hot=sess.run(train_pos_docs_as_one_hot_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_docs_as_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./train_neg_docs_as_one_hot', train_neg_docs_as_one_hot)\n",
    "np.save('./train_pos_docs_as_one_hot', train_pos_docs_as_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_docs_as_one_hot_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
